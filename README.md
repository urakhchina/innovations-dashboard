# 2025 New Product Launch Analytics Dashboard

Automated analytics dashboard tracking Irwin Naturals' 2025 new product launches with real-time data from AWS RDS.

## Features

- **Automated Data Refresh**: Nightly updates from AWS RDS via GitHub Actions
- **ML-Powered Insights**: Churn prediction and sales forecasting
- **Interactive Dashboard**: Real-time performance tracking for new launches
- **Zero Manual Work**: Set it once, forget it

## Architecture

```
Source: AWS RDS (ebdb) → Transform: SQL + Python → Deploy: Vercel
                 ↓
          GitHub Actions (nightly)
                 ↓
         CSV Files (data/) → index.html
```

## Setup

### Prerequisites

- PostgreSQL client (`psql`)
- Python 3.8+
- Access to AWS RDS database
- GitHub repository access
- Vercel account (for hosting)

### 1. Clone & Install

```bash
git clone <your-repo-url>
cd Innovations
pip install -r requirements.txt
```

### 2. Configure Database Connection

Set your AWS RDS connection string:

```bash
export PSQL_DSN='postgresql://username:password@your-rds-host:5432/ebdb?sslmode=require'
```

**Important**: Never commit credentials! Use environment variables or secrets.

### 3. Test Locally

Pull fresh data from RDS:

```bash
./scripts/pull_data.sh
```

Run analytics pipelines:

```bash
./scripts/run_pipelines.sh
```

View the dashboard:

```bash
# Open index.html in your browser
open index.html
```

### 4. Configure GitHub Secrets

In your GitHub repository, add these secrets:

1. **PSQL_DSN**: Your full RDS connection string
   ```
   postgresql://username:password@host:5432/ebdb?sslmode=require
   ```

2. **VERCEL_HOOK_URL** (optional): Vercel deploy hook URL
   - Go to Vercel → Project Settings → Git → Deploy Hooks
   - Create hook named "Innovations Auto-Refresh"
   - Copy the URL

### 5. Enable GitHub Actions

The workflow is configured to run:
- **Nightly** at 7:15 AM UTC (11:15 PM PST)
- **Manually** via "Actions" tab → "Refresh Innovations Data" → "Run workflow"

## Data Files

### Generated by SQL Queries

Located in `/data/`:

- **products_2025_by_upc.csv** - Master transaction file
- **summary_monthly.csv** - Monthly sales aggregates
- **summary_distributor.csv** - Distributor performance
- **summary_salesrep.csv** - Sales rep performance
- **summary_accounts_top50.csv** - Top 50 accounts per product
- **top_10_overall_skus.csv** - Top 10 SKUs with growth
- **launches_performance.csv** - New product launch tracking

### Generated by Python Pipelines

- **churn_predictions_60d.csv** - Account reorder predictions (60-day horizon)
- Forecast files (if forecast pipeline is enabled)

## SQL Queries

All queries are in `/sql/` directory. Each uses `COPY ... TO STDOUT WITH CSV HEADER` for clean output.

### Key Queries

- **products_2025_by_upc.sql**: Main data pull with UPC normalization
- **top_10_overall_skus.sql**: Top performers with YoY growth
- **launches_performance.sql**: Tracks 2025 new product launches

### Customization

To add/modify new product launches, edit `/sql/launches_performance.sql`:

```sql
WITH launch_list(product_label, upc_dashed) AS (
  VALUES
    ('Product Name', 'UPC-Code'),
    -- Add more here
)
```

## Troubleshooting

### Database Connection Issues

```bash
# Test connection
psql "$PSQL_DSN" -c "SELECT COUNT(*) FROM transactions WHERE posting_date >= '2025-01-01'"
```

### Pipeline Errors

Check logs in GitHub Actions or run locally:

```bash
# Verbose mode
set -x
./scripts/pull_data.sh
./scripts/run_pipelines.sh
```

### Missing Data Files

Ensure SQL files are executable:

```bash
chmod +x scripts/*.sh
```

## Manual Refresh

To manually refresh data:

```bash
# Pull latest data + run pipelines
export PSQL_DSN='postgresql://...'
./scripts/pull_data.sh && ./scripts/run_pipelines.sh

# Commit and push
git add data/*.csv
git commit -m "Manual data refresh $(date '+%Y-%m-%d')"
git push
```

## Deployment

### Vercel

1. Connect repository to Vercel
2. Framework Preset: **Other**
3. Build Command: (leave empty - static site)
4. Output Directory: `./`
5. Deploy!

Every push to `main` with updated CSVs will trigger a rebuild.

## Development

### Adding New SQL Queries

1. Create `/sql/your_query.sql` with `COPY (...) TO STDOUT WITH CSV HEADER`
2. Add to `/scripts/pull_data.sh`:
   ```bash
   run_sql sql/your_query.sql data/your_data.csv
   ```
3. Update dashboard HTML to load the new CSV

### Modifying Pipelines

Python pipelines are in the root directory:
- **churn_reorder_pipeline.py** - ML churn prediction
- **sales_forecast_pipeline.py** - Time series forecasting

## Support

For issues, check:
1. GitHub Actions logs
2. Vercel deployment logs
3. Database connection (firewall/VPN)

## License

Internal use - Irwin Naturals HFS Analytics
